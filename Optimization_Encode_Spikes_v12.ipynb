{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "<img src=\"https://www.uoc.edu/content/dam/news/images/noticies/2016/202-nova-marca-uoc.jpg\", align=\"left\">\n",
        "</div>\n",
        "<div style=\"float: right; width: 50%;\">\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.879 · TFM - Área 2 - Aula 1</p>\n",
        "<p style=\"margin: 0; text-align:right;\">2023-2 · Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
        "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>\n"
      ],
      "metadata": {
        "id": "dyHeq4b3L9uS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carga, preprocesamiento, aumento y codificación de imágenes en impulsos (Spikes)**\n",
        "\n",
        "## Introducción\n",
        "\n",
        "El presente código está diseñado para transformar un conjunto de datos de imágenes de conducción autónoma en un formato adecuado para ser utilizado en redes neuronales de impulsos (SNN), específicamente codificando las imágenes en impulsos utilizando la técnica de modulación delta de la librería [snnTorch](https://snntorch.readthedocs.io/en/latest/index.html). Este proceso es fundamental para entrenar una red neuronal convolucional de impulsos (CSNN) que pueda emular el comportamiento de redes neuronales biológicas, aprovechando su eficiencia energética y su capacidad para procesar información temporal.\n",
        "\n",
        "El código se estructura en varias etapas que abarcan desde la carga y preprocesamiento de datos, hasta la codificación de las imágenes y la medición de la eficiencia energética del proceso. A continuación se detallan las principales etapas y funcionalidades del código:\n",
        "\n",
        "1. **Montar Google Drive y Descomprimir el Dataset**: Se monta Google Drive para acceder a los datos y se descomprime el archivo del dataset de entrenamiento.\n",
        "   \n",
        "2. **Instalación de Librerías**: Se instalan las librerías necesarias para el procesamiento y codificación de imágenes, así como para el seguimiento de emisiones de carbono.\n",
        "\n",
        "3. **Importación de Librerías**: Se importan las librerías necesarias para la manipulación de datos, procesamiento de imágenes, codificación en impulsos y seguimiento de emisiones de carbono.\n",
        "\n",
        "4. **Funciones de Carga y Preprocesamiento de Datos**: Se definen funciones para cargar los datos de conducción desde archivos CSV, simplificar rutas de imágenes, categorizar ángulos de giro, equilibrar el dataset y dividirlo en conjuntos de entrenamiento y validación.\n",
        "\n",
        "5. **Funciones de Aumento de Datos**: Se implementan diversas técnicas de aumento de datos, como zoom, volteo, pan, brillo y sombras aleatorias, para aumentar la variedad del dataset y mejorar el entrenamiento del modelo.\n",
        "\n",
        "6. **Funciones de Preprocesamiento de Imágenes y Codificación de Impulsos**: Se preprocesan las imágenes para convertirlas en un formato adecuado para la codificación en impulsos, normalizan los ángulos de dirección y realizan la codificación delta utilizando la librería `snntorch`.\n",
        "\n",
        "7. **Carga y Balanceo de Datos de Conducción**: Se carga y balancea el dataset de conducción para crear conjuntos de entrenamiento y validación.\n",
        "\n",
        "8. **Codificación de Datos y Guardado de Resultados**: Se inicia el tracker de CodeCarbon para medir las emisiones de carbono durante el proceso de codificación, se preparan y codifican los datos de entrenamiento y validación, y se guardan los resultados en archivos HDF5. Finalmente, se copian los archivos a Google Drive.\n",
        "\n",
        "Este enfoque permite no solo la preparación y transformación de los datos en un formato adecuado para SNN, sino también la evaluación de la eficiencia energética del proceso, lo cual es crucial para desarrollar soluciones sostenibles en el campo de la conducción autónoma."
      ],
      "metadata": {
        "id": "0zPFYXgPMROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Montar Google Drive y Descomprimir el Dataset**: Montamos Google Drive y descomprimimos el dataset que contiene las imágenes de entrenamiento."
      ],
      "metadata": {
        "id": "qT6ys9oGOqof"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RRMPb1nqovK",
        "outputId": "f6edbce1-b780-48a1-a072-5c0bcafd0120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip \"/content/drive/MyDrive/UOC/TFM/Dataset/TrainImagesTFM_1_2.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "40s4XKklq32z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Instalación de Librerías**: Instalamos las librerías necesarias para el procesamiento y codificación de las imágenes, así como para medir la eficiencia energética."
      ],
      "metadata": {
        "id": "iteQVz2YPCXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install codecarbon torch snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5rp_zO0q6KZ",
        "outputId": "c158d4ce-0bd9-4b1b-bb1b-5bc9b83889ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.5.0-py3-none-any.whl (496 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/496.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m491.5/496.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.1/496.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.0.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Collecting pynvml (from codecarbon)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.25.2)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.4-py3-none-any.whl (18 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Installing collected packages: types-python-dateutil, rapidfuzz, pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nir, arrow, nvidia-cusolver-cu12, codecarbon, nirtorch, snntorch\n",
            "Successfully installed arrow-1.3.0 codecarbon-2.5.0 nir-1.0.4 nirtorch-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pynvml-11.5.0 rapidfuzz-3.9.3 snntorch-0.9.1 types-python-dateutil-2.9.0.20240316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Importación de Librerías**: Importamos las librerías necesarias para manipulación de datos, procesamiento de imágenes, codificación de impulsos y seguimiento de emisiones de carbono."
      ],
      "metadata": {
        "id": "VvcbzNx6PQyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from imgaug import augmenters as iaa\n",
        "import os\n",
        "import h5py\n",
        "import snntorch.spikegen as spikegen\n",
        "from tqdm import tqdm\n",
        "from codecarbon import EmissionsTracker\n",
        "import cv2\n",
        "import matplotlib.image as mpimg\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import shutil"
      ],
      "metadata": {
        "id": "LVkZEcgCq9C6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Funciones de Carga y Preprocesamiento de Datos**: Estas funciones se utilizan para cargar y preprocesar los datos de conducción desde archivos CSV, equilibrar el dataset y dividirlo en conjuntos de entrenamiento y validación."
      ],
      "metadata": {
        "id": "ppI-Gph6PY8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos de conducción desde un archivo CSV\n",
        "def load_driving_data(csv_file_path):\n",
        "    data = pd.read_csv(csv_file_path)\n",
        "    data.columns = [\n",
        "        'center_image', 'left_image', 'right_image',\n",
        "        'steering_angle', 'throttle', 'brake', 'speed'\n",
        "    ]\n",
        "    return data\n",
        "\n",
        "# Simplificar las rutas de las imágenes en el DataFrame\n",
        "def simplify_image_path(dataframe, columns):\n",
        "    for column in columns:\n",
        "        dataframe[column] = dataframe[column].apply(lambda x: x.split('/')[-1])\n",
        "    return dataframe\n",
        "\n",
        "# Categorizar los ángulos de giro en el DataFrame\n",
        "def categorize_turns(data, angle_column):\n",
        "    bins = [-float('inf'), -0.1, 0.1, float('inf')]\n",
        "    labels = ['Left Turn', 'Straight', 'Right Turn']\n",
        "    data['turn_category'] = pd.cut(data[angle_column], bins=bins, labels=labels)\n",
        "    turn_counts = data['turn_category'].value_counts()\n",
        "    return turn_counts\n",
        "\n",
        "# Equilibrar los datos replicando registros en categorías menos representadas\n",
        "def balance_data_by_replication(data, category_column, target_size=None):\n",
        "    categories = data[category_column].unique()\n",
        "    subset_list = [data[data[category_column] == category] for category in categories]\n",
        "    if not target_size:\n",
        "        target_size = max(subset.shape[0] for subset in subset_list)\n",
        "    balanced_subsets = [subset.sample(target_size, replace=True) if subset.shape[0] < target_size else subset for subset in subset_list]\n",
        "    balanced_data = pd.concat(balanced_subsets, ignore_index=True)\n",
        "    return balanced_data\n",
        "\n",
        "# Calcular el número máximo de muestras por bin basado en un percentil dado\n",
        "def calculate_samples_per_bin(df, steering_col, num_bins, percentile):\n",
        "    hist, _ = np.histogram(df[steering_col], bins=num_bins)\n",
        "    max_samples = np.percentile(hist, percentile)\n",
        "    return int(max_samples)\n",
        "\n",
        "# Ajustar la distribución de los ángulos de dirección para que sea más uniforme por categoría de giro\n",
        "def remove_samples_by_category(df, category_col='turn_category', steering_col='steering_angle', num_bins=25, samples_per_bin=None):\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    categories = df[category_col].unique()\n",
        "    balanced_dfs = []\n",
        "\n",
        "    for category in categories:\n",
        "        category_df = df[df[category_col] == category]\n",
        "        hist, bins = np.histogram(category_df[steering_col], bins=num_bins)\n",
        "        if samples_per_bin is None:\n",
        "            samples_per_bin = int(np.percentile(hist, 90))\n",
        "        remove_list = []\n",
        "\n",
        "        for j in range(num_bins):\n",
        "            list_indices = [i for i in range(len(category_df[steering_col])) if bins[j] <= category_df[steering_col].iloc[i] < bins[j + 1]]\n",
        "            list_indices = shuffle(list_indices)\n",
        "            if len(list_indices) > samples_per_bin:\n",
        "                remove_list.extend(list_indices[samples_per_bin:])\n",
        "\n",
        "        category_df = category_df.drop(category_df.index[remove_list])\n",
        "        balanced_dfs.append(category_df)\n",
        "\n",
        "    return pd.concat(balanced_dfs, ignore_index=True)\n",
        "\n",
        "# Asocia cada imagen de las cámaras central, izquierda y derecha con un ángulo de giro ajustado\n",
        "def load_img_steering(datadir, df):\n",
        "    image_path = []\n",
        "    steering = []\n",
        "    for i in range(len(df)):\n",
        "        indexed_data = df.iloc[i]\n",
        "        center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n",
        "        image_path.append(os.path.join(datadir, center.strip()))\n",
        "        steering.append(float(indexed_data[3]))\n",
        "        image_path.append(os.path.join(datadir, left.strip()))\n",
        "        steering.append(float(indexed_data[3]) + 0.2)\n",
        "        image_path.append(os.path.join(datadir, right.strip()))\n",
        "        steering.append(float(indexed_data[3]) - 0.2)\n",
        "    return np.array(image_path), np.array(steering)\n",
        "\n",
        "# Crea datos de entrenamiento y validación\n",
        "def trainValSample(dir, df):\n",
        "    if not dir.endswith(\"/\"):\n",
        "        dir += \"/\"\n",
        "    image_paths, steerings = load_img_steering(dir + \"IMG/\", df)\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steerings, test_size=0.2, random_state=42)\n",
        "    print(\"Training Samples: {}\\nValidation Samples: {}\".format(len(X_train), len(X_valid)))\n",
        "    print(\"Total Samples: {}\".format(len(image_paths)))\n",
        "    return image_paths, steerings, X_train, X_valid, y_train, y_valid"
      ],
      "metadata": {
        "id": "K7um2gHNPoLl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Funciones de Aumento de Datos**: Estas funciones aplican diversas técnicas de aumento de datos, como zoom, volteo, pan, brillo y sombras aleatorias, para aumentar la variedad del dataset y mejorar el entrenamiento del modelo."
      ],
      "metadata": {
        "id": "bIEPx2azPuYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para convertir la imagen a escala de grises\n",
        "def convert_to_grayscale(image):\n",
        "    if image is None or image.size == 0:\n",
        "        raise ValueError(\"Empty image provided for grayscale conversion.\")\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Funciones de aumento de imagen\n",
        "def apply_zoom(image):\n",
        "    zoom_transformer = iaa.Affine(scale=(1, 1.5))\n",
        "    zoomed_image = zoom_transformer.augment_image(image)\n",
        "    return zoomed_image\n",
        "\n",
        "def random_flip(image, steering_angle):\n",
        "    flipped_image = cv2.flip(image, 1)\n",
        "    flipped_steering_angle = -steering_angle\n",
        "    return flipped_image, flipped_steering_angle\n",
        "\n",
        "def apply_pan(image):\n",
        "    pan_transformer = iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
        "    panned_image = pan_transformer.augment_image(image)\n",
        "    return panned_image\n",
        "\n",
        "def apply_random_brightness(image):\n",
        "    brightness_augmenter = iaa.Multiply((0.2, 1.6))\n",
        "    brightened_image = brightness_augmenter.augment_image(image)\n",
        "    return brightened_image\n",
        "\n",
        "def apply_image_brighten(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)  # Convertir a BGR para trabajar en el espacio HSV\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    brightness = 0.25 + np.random.uniform()\n",
        "    hsv_image[:, :, 2] = hsv_image[:, :, 2] * brightness\n",
        "    hsv_image[:, :, 2] = np.clip(hsv_image[:, :, 2], 0, 255)\n",
        "    brightened_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
        "    brightened_image = cv2.cvtColor(brightened_image, cv2.COLOR_BGR2GRAY)  # Convertir de vuelta a escala de grises\n",
        "    return brightened_image\n",
        "\n",
        "def apply_random_shadow(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)  # Convertir a BGR para trabajar en el espacio HLS\n",
        "    IMAGE_HEIGHT, IMAGE_WIDTH = image.shape[:2]\n",
        "    x1, y1 = IMAGE_WIDTH * np.random.rand(), 0\n",
        "    x2, y2 = IMAGE_WIDTH * np.random.rand(), IMAGE_HEIGHT\n",
        "    xm, ym = np.mgrid[0:IMAGE_HEIGHT, 0:IMAGE_WIDTH]\n",
        "    mask = np.zeros_like(image[:, :, 1])\n",
        "    mask[((ym - y1) * (x2 - x1) - (y2 - y1) * (xm - x1)) > 0] = 1\n",
        "    cond = mask == np.random.randint(2)\n",
        "    s_ratio = np.random.uniform(low=0.2, high=0.5)\n",
        "    hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n",
        "    hls[:, :, 1][cond] = hls[:, :, 1][cond] * s_ratio\n",
        "    shadowed_image = cv2.cvtColor(hls, cv2.COLOR_HLS2BGR)\n",
        "    shadowed_image = cv2.cvtColor(shadowed_image, cv2.COLOR_BGR2GRAY)  # Convertir de vuelta a escala de grises\n",
        "    return shadowed_image\n",
        "\n",
        "# Función para aplicar aumentos aleatorios y ajustar el tamaño de la imagen\n",
        "def random_augment(image, steering_angle):\n",
        "    image = convert_to_grayscale(image)  # Convertir la imagen a escala de grises\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = apply_pan(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = apply_zoom(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = apply_random_brightness(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = apply_image_brighten(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = apply_random_shadow(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image, steering_angle = random_flip(image, steering_angle)\n",
        "    return image, steering_angle"
      ],
      "metadata": {
        "id": "1ezAWzJxP7sp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Funciones de Preprocesamiento de Imágenes y Codificación de Impulsos**: Estas funciones preprocesan las imágenes para convertirlas en un formato adecuado para la codificación en impulsos, normalizan los ángulos de dirección y realizan la codificación delta utilizando la librería snntorch"
      ],
      "metadata": {
        "id": "ZyLwRNJ-QFvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para preprocesar la imagen para el entrenamiento del modelo CSNN\n",
        "def img_preprocess(img):\n",
        "    img = img[40:140, :]  # Recortar la imagen para eliminar características innecesarias\n",
        "    if len(img.shape) == 3:  # Verificar si la imagen tiene 3 canales\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convertir a escala de grises\n",
        "    img = cv2.GaussianBlur(img, (3, 3), 0)  # Aplicar desenfoque gaussiano\n",
        "    img = cv2.resize(img, (160, 50), interpolation=cv2.INTER_AREA)  # Reducir el tamaño para facilitar el procesamiento\n",
        "    img = (img - 128.) / 128.  # Normalizar los valores de los píxeles\n",
        "    return img\n",
        "\n",
        "# Función para convertir un valor a tensor\n",
        "def value_to_tensor(value, tensor_length=21):\n",
        "    value = round(value, 1)\n",
        "    if value < -1 or value > 1:\n",
        "        raise ValueError(\"El valor debe estar en el rango [-1, 1]\")\n",
        "    position = int(((value + 1) / 2) * (tensor_length - 1))\n",
        "    tensor = torch.zeros(tensor_length, dtype=torch.int32)\n",
        "    tensor[position] = 1\n",
        "    return tensor\n",
        "\n",
        "# Función para asegurar que el ángulo esté en el rango [-1, 1]\n",
        "def clip_steering_angle(angle):\n",
        "    return np.clip(angle, -1, 1)\n",
        "\n",
        "# Codificación Delta usando snnTorch\n",
        "def encode_delta(image1, image2):\n",
        "    if not isinstance(image1, torch.Tensor):\n",
        "        image1 = torch.tensor(image1, dtype=torch.float32)\n",
        "    if not isinstance(image2, torch.Tensor):\n",
        "        image2 = torch.tensor(image2, dtype=torch.float32)\n",
        "    delta_encoded = spikegen.delta(image1, image2)\n",
        "    return delta_encoded\n",
        "\n",
        "# Función para preparar y codificar los datos de entrenamiento\n",
        "def prepare_and_encode_train_data(image_paths, steering_angles, datadir, device='cuda'):\n",
        "    positions = []\n",
        "    delta_images = []\n",
        "    angles = []\n",
        "\n",
        "    # Ajustar el límite para asegurar que tengamos un número par de imágenes\n",
        "    limit = len(image_paths) - (len(image_paths) % 2)\n",
        "\n",
        "    for i in range(0, limit - 1, 2):\n",
        "        image_path1, image_path2 = image_paths[i], image_paths[i+1]\n",
        "        steering_angle2 = clip_steering_angle(steering_angles[i+1])\n",
        "\n",
        "        full_image_path1 = os.path.join(datadir, image_path1)\n",
        "        full_image_path2 = os.path.join(datadir, image_path2)\n",
        "\n",
        "        image1 = mpimg.imread(full_image_path1)\n",
        "        image2 = mpimg.imread(full_image_path2)\n",
        "\n",
        "        image1, _ = random_augment(image1, clip_steering_angle(steering_angles[i]))\n",
        "        image2, steering_angle2 = random_augment(image2, steering_angle2)\n",
        "\n",
        "        image1 = img_preprocess(image1)\n",
        "        image2 = img_preprocess(image2)\n",
        "\n",
        "        image1_tensor = torch.tensor(image1, dtype=torch.float32, device=device)\n",
        "        image2_tensor = torch.tensor(image2, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Asegurarse de que la codificación delta se aplica correctamente\n",
        "        delta_image = encode_delta(image1_tensor, image2_tensor)\n",
        "\n",
        "        # Añadir datos a las listas\n",
        "        positions.append(i // 2)\n",
        "        delta_images.append(delta_image)\n",
        "        angles.append(value_to_tensor(steering_angle2).to(device))\n",
        "\n",
        "    positions_tensor = torch.tensor(positions, dtype=torch.int32, device=device)\n",
        "    delta_images_tensor = torch.stack(delta_images)\n",
        "    angles_tensor = torch.stack(angles)\n",
        "\n",
        "    return positions_tensor, delta_images_tensor, angles_tensor\n",
        "\n",
        "# Función para preparar y codificar los datos de validación (sin aumentos)\n",
        "def prepare_and_encode_valid_data(image_paths, steering_angles, datadir, device='cuda'):\n",
        "    positions = []\n",
        "    delta_images = []\n",
        "    angles = []\n",
        "\n",
        "    # Ajustar el límite para asegurar que tengamos un número par de imágenes\n",
        "    limit = len(image_paths) - (len(image_paths) % 2)\n",
        "\n",
        "    for i in range(0, limit - 1, 2):\n",
        "        image_path1, image_path2 = image_paths[i], image_paths[i+1]\n",
        "        steering_angle2 = clip_steering_angle(steering_angles[i+1])\n",
        "\n",
        "        full_image_path1 = os.path.join(datadir, image_path1)\n",
        "        full_image_path2 = os.path.join(datadir, image_path2)\n",
        "\n",
        "        image1 = mpimg.imread(full_image_path1)\n",
        "        image2 = mpimg.imread(full_image_path2)\n",
        "\n",
        "        image1 = img_preprocess(image1)\n",
        "        image2 = img_preprocess(image2)\n",
        "\n",
        "        image1_tensor = torch.tensor(image1, dtype=torch.float32, device=device)\n",
        "        image2_tensor = torch.tensor(image2, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Asegurarse de que la codificación delta se aplica correctamente\n",
        "        delta_image = encode_delta(image1_tensor, image2_tensor)\n",
        "\n",
        "        # Añadir datos a las listas\n",
        "        positions.append(i // 2)\n",
        "        delta_images.append(delta_image)\n",
        "        angles.append(value_to_tensor(steering_angle2).to(device))\n",
        "\n",
        "    positions_tensor = torch.tensor(positions, dtype=torch.int32, device=device)\n",
        "    delta_images_tensor = torch.stack(delta_images)\n",
        "    angles_tensor = torch.stack(angles)\n",
        "\n",
        "    return positions_tensor, delta_images_tensor, angles_tensor"
      ],
      "metadata": {
        "id": "6ILMm-azQm5J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Carga y Balanceo de Datos de Conducción**: Cargamos y balanceamos los datos de conducción para crear conjuntos de entrenamiento y validación."
      ],
      "metadata": {
        "id": "nI-qaGbKQst2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos de entrenamiento y validación\n",
        "csv_file_path = '/content/TrainImagesTFM_1_2/driving_log.csv'\n",
        "driving_data = load_driving_data(csv_file_path)\n",
        "driving_data = simplify_image_path(driving_data, ['center_image', 'left_image', 'right_image'])\n",
        "turn_counts = categorize_turns(driving_data, 'steering_angle')\n",
        "balanced_driving_data = balance_data_by_replication(driving_data, 'turn_category')\n",
        "percentile = 90\n",
        "num_bins = 25\n",
        "max_samples_per_bin = calculate_samples_per_bin(balanced_driving_data, 'steering_angle', num_bins, percentile)\n",
        "balanced_data_adjusted = remove_samples_by_category(balanced_driving_data, samples_per_bin=max_samples_per_bin)\n",
        "image_paths, steerings, X_train, X_valid, y_train, y_valid = trainValSample(\"/content/TrainImagesTFM_1_2/\", balanced_data_adjusted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgfbRCDfQ4FX",
        "outputId": "c56ef584-66e0-489b-b599-10244302e532"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Samples: 44179\n",
            "Validation Samples: 11045\n",
            "Total Samples: 55224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Codificación de Datos y Guardado de Resultados**: Iniciamos el tracker de CodeCarbon para medir las emisiones de carbono durante el proceso de codificación, preparamos y codificamos los datos de entrenamiento y validación, y guardamos los resultados en archivos HDF5. Finalmente, copiamos los archivos a Google Drive."
      ],
      "metadata": {
        "id": "zWI7dS7iQ7Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dado que X_train, y_train, X_valid, y_valid están definidos\n",
        "datadir = \"/content/TrainImagesTFM_1_2/\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Capturar información del tiempo para guardado\n",
        "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Iniciar el tracker de CodeCarbon\n",
        "tracker = EmissionsTracker(output_dir='/content/drive/MyDrive/UOC/TFM/EcoEfficiency_Reports/', project_name=f\"emissions_{current_time}.csv\")\n",
        "tracker.start()\n",
        "\n",
        "# Procesar los conjuntos de datos de entrenamiento y validación\n",
        "train_positions, train_delta_images, train_angles = prepare_and_encode_train_data(X_train, y_train, datadir, device)\n",
        "valid_positions, valid_delta_images, valid_angles = prepare_and_encode_valid_data(X_valid, y_valid, datadir, device)\n",
        "\n",
        "# Verificar las dimensiones de los tensores generados\n",
        "print(f\"Dimensiones del tensor de posiciones de entrenamiento: {train_positions.shape}\")\n",
        "print(f\"Dimensiones del tensor de imágenes delta de entrenamiento: {train_delta_images.shape}\")\n",
        "print(f\"Dimensiones del tensor de ángulos de entrenamiento: {train_angles.shape}\")\n",
        "\n",
        "print(f\"Dimensiones del tensor de posiciones de validación: {valid_positions.shape}\")\n",
        "print(f\"Dimensiones del tensor de imágenes delta de validación: {valid_delta_images.shape}\")\n",
        "print(f\"Dimensiones del tensor de ángulos de validación: {valid_angles.shape}\")\n",
        "\n",
        "# Guardar en formato HDF5\n",
        "def save_to_hdf5(positions, delta_images, angles, file_name):\n",
        "    with h5py.File(file_name, 'w') as f:\n",
        "        f.create_dataset('positions', data=positions.cpu().numpy(), compression=\"gzip\")\n",
        "        f.create_dataset('delta_images', data=delta_images.cpu().numpy(), compression=\"gzip\")\n",
        "        f.create_dataset('angles', data=angles.cpu().numpy(), compression=\"gzip\")\n",
        "\n",
        "save_to_hdf5(train_positions, train_delta_images, train_angles, f'/content/training_data_pairs_{current_time}.hdf5')\n",
        "save_to_hdf5(valid_positions, valid_delta_images, valid_angles, f'/content/validation_data_pairs_{current_time}.hdf5')\n",
        "\n",
        "tracker.stop()\n",
        "\n",
        "# Copiar archivos a Google Drive\n",
        "shutil.copy2(f'/content/training_data_pairs_{current_time}.hdf5', f'/content/drive/MyDrive/UOC/TFM/Spike Encoding/Version_10/training_data_pairs_{current_time}.hdf5')\n",
        "shutil.copy2(f'/content/validation_data_pairs_{current_time}.hdf5', f'/content/drive/MyDrive/UOC/TFM/Spike Encoding/Version_10/validation_data_pairs_{current_time}.hdf5')\n",
        "\n",
        "print(f\"Total de pares de imágenes codificadas y combinadas en entrenamiento: {train_positions.shape[0]}\")\n",
        "print(f\"Total de pares de imágenes codificadas y combinadas en validación: {valid_positions.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y08v14SZ1rpL",
        "outputId": "31d3c6d5-ceec-46dd-d1d1-e5d0cb959c05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 19:05:39] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 19:05:39] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 19:05:39] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 19:05:39] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 19:05:39] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon WARNING @ 19:05:40] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 19:05:40] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 19:05:40] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 19:05:40]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 19:05:40]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 19:05:40]   CodeCarbon version: 2.5.0\n",
            "[codecarbon INFO @ 19:05:40]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 19:05:40]   CPU count: 2\n",
            "[codecarbon INFO @ 19:05:40]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 19:05:40]   GPU count: 1\n",
            "[codecarbon INFO @ 19:05:40]   GPU model: 1 x Tesla T4\n",
            "[codecarbon WARNING @ 19:05:41] Unable to access geographical location through primary API. Will resort to using the backup API - Exception : HTTPSConnectionPool(host='get.geojs.io', port=443): Read timed out. (read timeout=0.5) - url=https://get.geojs.io/v1/ip/geo.json\n",
            "[codecarbon WARNING @ 19:05:41] Unable to access geographical location. Using 'Canada' as the default value - Exception : 'country' - url=https://get.geojs.io/v1/ip/geo.json\n",
            "[codecarbon INFO @ 19:05:41] Saving emissions data to file /content/drive/MyDrive/UOC/TFM/EcoEfficiency_Reports/emissions.csv\n",
            "[codecarbon INFO @ 19:05:56] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:05:56] Energy consumed for all GPUs : 0.000118 kWh. Total GPU Power : 28.36378762453141 W\n",
            "[codecarbon INFO @ 19:05:56] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:05:56] 0.000315 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:06:11] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:06:11] Energy consumed for all GPUs : 0.000240 kWh. Total GPU Power : 29.19441303104368 W\n",
            "[codecarbon INFO @ 19:06:11] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:06:11] 0.000634 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:06:26] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:06:26] Energy consumed for all GPUs : 0.000361 kWh. Total GPU Power : 29.130280721575804 W\n",
            "[codecarbon INFO @ 19:06:26] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:06:26] 0.000952 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:06:41] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:06:41] Energy consumed for all GPUs : 0.000483 kWh. Total GPU Power : 29.28269001124824 W\n",
            "[codecarbon INFO @ 19:06:41] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:06:41] 0.001271 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:06:56] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:06:56] Energy consumed for all GPUs : 0.000606 kWh. Total GPU Power : 29.619533002190728 W\n",
            "[codecarbon INFO @ 19:06:56] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:06:56] 0.001591 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:07:11] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:07:11] Energy consumed for all GPUs : 0.000730 kWh. Total GPU Power : 29.628576349272812 W\n",
            "[codecarbon INFO @ 19:07:11] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:07:11] 0.001911 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:07:26] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:07:26] Energy consumed for all GPUs : 0.000854 kWh. Total GPU Power : 29.706340098965658 W\n",
            "[codecarbon INFO @ 19:07:26] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:07:26] 0.002232 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:07:41] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:07:41] Energy consumed for all GPUs : 0.000977 kWh. Total GPU Power : 29.63358749639776 W\n",
            "[codecarbon INFO @ 19:07:41] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:07:41] 0.002552 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:07:41] 0.000051 g.CO2eq/s mean an estimation of 1.5938661023713663 kg.CO2eq/year\n",
            "[codecarbon INFO @ 19:07:56] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:07:56] Energy consumed for all GPUs : 0.001102 kWh. Total GPU Power : 29.976419521112422 W\n",
            "[codecarbon INFO @ 19:07:56] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:07:56] 0.002874 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:08:11] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:08:11] Energy consumed for all GPUs : 0.001226 kWh. Total GPU Power : 29.888677480934653 W\n",
            "[codecarbon INFO @ 19:08:11] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:08:11] 0.003195 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:08:26] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:08:26] Energy consumed for all GPUs : 0.001351 kWh. Total GPU Power : 29.97738248556952 W\n",
            "[codecarbon INFO @ 19:08:26] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:08:26] 0.003516 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:08:41] Energy consumed for RAM : 0.000237 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:08:41] Energy consumed for all GPUs : 0.001476 kWh. Total GPU Power : 30.065073593187016 W\n",
            "[codecarbon INFO @ 19:08:41] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:08:41] 0.003838 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones del tensor de posiciones de entrenamiento: torch.Size([22089])\n",
            "Dimensiones del tensor de imágenes delta de entrenamiento: torch.Size([22089, 50, 160])\n",
            "Dimensiones del tensor de ángulos de entrenamiento: torch.Size([22089, 21])\n",
            "Dimensiones del tensor de posiciones de validación: torch.Size([5522])\n",
            "Dimensiones del tensor de imágenes delta de validación: torch.Size([5522, 50, 160])\n",
            "Dimensiones del tensor de ángulos de validación: torch.Size([5522, 21])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 19:08:56] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:08:56] Energy consumed for all GPUs : 0.001601 kWh. Total GPU Power : 30.079081764785837 W\n",
            "[codecarbon INFO @ 19:08:56] Energy consumed for all CPUs : 0.002302 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:08:56] 0.004160 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:08:59] Energy consumed for RAM : 0.000260 kWh. RAM Power : 4.753040313720703 W\n",
            "[codecarbon INFO @ 19:08:59] Energy consumed for all GPUs : 0.001622 kWh. Total GPU Power : 30.504508883385085 W\n",
            "[codecarbon INFO @ 19:08:59] Energy consumed for all CPUs : 0.002330 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 19:08:59] 0.004212 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de pares de imágenes codificadas y combinadas en entrenamiento: 22089\n",
            "Total de pares de imágenes codificadas y combinadas en validación: 5522\n"
          ]
        }
      ]
    }
  ]
}